# HDFS 分布式文件系统
HDFS是Hadoop大数据平台的最底层，其实现以下目标:
1. 兼容**廉价**的硬件设备
2. 流数据读写
3. 大数据集
4. 简单的文件模型
5. 强大的跨平台兼容性(Java)

上述目标带来优良特性的同时，也使其有一定**局限性**:
1. **不适合低延迟**数据访问
2. 无法高效存储**小文件**
3. 不支持**多用户写入**及任意**修改**文件

## HDFS存储模型
### 块
HDFS以块为存储单位，块的大小默认为64MB，一个文件可以被分成多个块存储在HDFS不同节点上，使用抽象的块概念可以带来以下几个明显的好处:
1. **支持大规模文件存储**：  
一个大规模的文件可以被拆分为若干个文件快，不同的文件快会被分发到不同的节点上去。因此，存储文件的大小不会受到单个节点存储容量的限制，无论多么大的文件都可以被切分成文件块，分发到不同的节点上去，那么只要整个集群的存储能力足够强大，无论多么大的文件都可以被存储在HDFS中。  
2. **简化系统设计**：  
首先，大大简化了存储管理，因为文件块大小是固定的，这样就很容易计算出一个结点能存储多少文件块；其次，方便了元数据的管理，元数据不需要和文件块一起存储，可以有其他系统管理元数据。  
3. **适合数据备份**：  
每个文件块都冗余存储(数据备份机制)在多个结点上，大大提高了系统的容错性和可用性。  
### 名称节点和数据节点
#### **名称节点**
1. 存储元数据；  
2. 元数据保存在内存之中；  
3. 保存文件树、数据节点与文件块之间的映射关系。  
#### **名称结点的数据结构**
1. FsImage：  
FsImage用于维护和保存**系统文件树**以及文件树中所有文件和文件夹的**元数据**，包括文件的复制等级、修改和访问时间、访问权限、快的大小以及组成文件的块。  
2. EditLog：记录了所有针对文件的创建、删除、重命名等操作。  
3. 但是FsImage和EditLog并没有保存数据块和节点之间的映射关系，NmaNode将这些映射关系做成一张表存储在内存之中，当数据节点加入HDFS集群时数据节点会定时把自己所包含的块列表告知NameNode(**热插拔**)，通过这种心跳机制以确保NameNode中{块->节点}的映射关系是最新的。
#### **名称节点的启动**
NameNode启动时，会将FsImage加载到内存中，然后再根据EditLog中记录的操作对FsImage进行修改，使其符合文件系统的现状，更新完之后，再创建一个空的EditLog。
#### **第二名称节点(SecondaryNameNode)**
SecondaryNameNode是用来保存名称节点中对HDFS 元数据信息的备份(**冷备份**)，并减少名称节点重启的时间。  
1. SecondaryNameNode会定期和名称节点进行通信，要求其停止使用EditLog，暂时将新的操作写道另一个日志文件edit.new上去；  
2. SecondaryNameNode通过HTTP get方式从NameNode上获取到FsImage文件和EditLog文件；  
3. SecondaryNameNode将FsImage加载到内存中，再通过EditLog中记录的操作对FsImage进行更新，得到FsImage.cpkt；  
4. 通过post方式将FsImage.cpkt文件发送给NameNode；  
5. NameNode在收到FsImage.cpkt之后，用FsImage.cpkt替换掉原来FsImage文件，同时用Edit.new替换掉原来的EditLog文件。  
借助SecondaryNameNode，我们成功对FsImage文件和EditLog文件进行了更新，且减小了EditLog所占的资源，解决了NameNode运行时EditLog无限膨胀的问题。
#### 数据节点
1. 用分块来存储文件的内容；  
2. 文件内容保存在各个结点的磁盘之中；  
3. 维护块与数据节点之间的映射关系。  
###  冗余数据保存 
1. 保证系统的可用性  
当有多个任务需要对同一个文件或者数据块进行使用时，NameNode可以将各不相同的副本分配给它们，这种机制就为系统提供了高可用性，同时也使得计算向数据移动。  
2. 保证系统的高容错性  
通过NameNode和DataNode之间的心跳机制（若某个DataNode发送的某些数据块的校验码与NameNode保存的校验码不相同，则可知这些数据块出现了错误），如果某一个数据块的内容发生错误，NameNode可以调用集群中的数据备份来修正错误。  
### 数据读取
1. 客户端向NameNode发送读取文件的申请；  
2. NameNode判断你的文件是否存在；  
3. 返回文件的数据块的位置信息（初次调用时返回的的是文件开始位置的数据块的信息）；  
4. 客户端收到存放block块的信息（DataNode列表）后，按照网络距离的近远进行排序，首先访问最近的DataNode，如果该节点上存放的block不可读取，则按照顺序访问下一个DataNode；  
5. 读取完毕后，断开数据连接，若有剩余的block需要读取，则重复2-5。
### 数据写入
1. 客户端向NameNode发送写入文件的申请；  
2. NameNode判断你的文件及其路径是否合法，合法就同意上传文件；  
3. 客户端将文件切分成文件块，客户端向NamaNode申请可用的数据节点；  
4. NameNode将可用的DataNode的信息（如DN1、DN2、DN3）发回给客户端；  
5. 客户端按照网络距离的远近对DataNode进行排序，然后将请求上传Block的消息发送给最近的DataNode，如DN1，DN1再将请求上传block的消息发送给DN2，DN2再将消息发送给DN3，确认消息将沿着DN3->DN2->DN1->client这样的路径返回到客户端，这样就够建好了一个数据传送的pipeline：client->DN1->DN2->DN3；  
6. 客户端通过package的方式发出block，package按照pipeline从DN1流到DN3，ack也按照原路返回；  
7. block传送完毕后，向client和NameNode发送传送完毕的消息，断开当前的数据连接，如果还有剩余的block需要上传，按照3-7的步骤再次上传block。